{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNL6Zwf5p5FxnmirfZrDkuD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/crdc_compound_objects/blob/main/Hierarchical_Instance_Names_s3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "\n",
        "In this notebook we investigate a proposed alternative to current IDC blob naming and its interaction with the proposed CRDC compound object.\n",
        "\n",
        "## IDC DICOM Instance Names\n",
        "\n",
        "IDC instance blobs are stored in GCS buckets and currently have a flat name space. Blobs have GCS names like `<instance_uuid>.dcm`. `<instance_uuid>` is the UUID4 assigned by IDC to a version of a DICOM instance.\n",
        "\n",
        "This notebook demonstrates an alternative to current IDC blob naming in which instance blobs would be named as `<series_uuid>/<instance_uuid>.dcm` and where we index series and instances. Here `<series_uuid>` is the UUID4 assigned by IDC to a version of a DICOM series. By definition, all the instances in a series are in a single bucket.\n",
        "  \n",
        "We demonstrate that, given such a naming convention, we do not need to resolve the DRS URI of individual instances when accessing publicly available instance blobs, that is, when signed URLs are not needed for access. Specifically, because gsutil and other libraries and utilities understand hierarchical naming, knowing a <series_uuid> and the name of the bucket in which the instances in that series reside, is sufficient in order to identify and/or access all the instances in a series.\n",
        "\n",
        "In the case that signed URLs are needed to access IDC instance data in a series, the DRS URI, which has the form `drs://dg.4DFC/<series_uuid> can resolved to obtain the GCS URL of an IDC series object. See the [CRDC compound object notebook](https://github.com/ImagingDataCommons/crdc_compound_objects/blob/main/CRDC_compound_object.ipynb)."
      ],
      "metadata": {
        "id": "Vswlv6Yd-25l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo\n",
        "\n",
        "We've populated a GCS bucket, `cdrcobj`, with a selected set of series from the [NLST collection](https://doi.org/10.7937/tcia.hmq8-j677).\n",
        "\n",
        "Instances have hierarchical names like `<series_uuid>/<instance_uuid>.dcm` as described above.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nn8k_uLKkaZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_bucket = 'crdcobj'"
      ],
      "metadata": {
        "id": "52g7MeAa11gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo config\n",
        "The jq JSON processor is useful for inspecting JSON:"
      ],
      "metadata": {
        "id": "ZB63N50uGLXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt -qq update\n",
        "!apt -qq install jq"
      ],
      "metadata": {
        "id": "Ip6E9LIhGcze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to access GCS, we first authenticate to Colab:"
      ],
      "metadata": {
        "id": "iX2msn_Cu-Rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "IA7SR0nVvN1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using the GCS Python libraries:"
      ],
      "metadata": {
        "id": "9fzsqyPyR9uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import json\n",
        "client = storage.Client()\n",
        "bucket = client.bucket(demo_bucket)"
      ],
      "metadata": {
        "id": "_oohwLeZN3ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install s5cmd."
      ],
      "metadata": {
        "id": "hq_2-6Sc1Ytd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/peak/s5cmd/releases/download/v2.0.0-beta/s5cmd_2.0.0-beta_Linux-64bit.tar.gz\n",
        "!tar zxf s5cmd_2.0.0-beta_Linux-64bit.tar.gz"
      ],
      "metadata": {
        "id": "S5fKG9_D1Z3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Series objects in the demo bucket\n",
        "\n",
        "Note that we have also created a series compound object for each series. Each has a GCS URL like:\n",
        "`gs://crdcobj/<series_uuid>/crdcobj.json`. E.G.:"
      ],
      "metadata": {
        "id": "s95_stTtLCq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cat gs://crdcobj/f5d6b517-2c02-4035-9444-0f15be7180ff/crdcobj.json | jq"
      ],
      "metadata": {
        "id": "Qgpc6iZ4Ja8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the CRDC compound cbject notebook for more details. We won't be further discussing these `series objects` in this notebook."
      ],
      "metadata": {
        "id": "7wpQnX02BKkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the instance name hierarchy\n",
        "\n",
        "As mentioned previously, instance blobs in the demo_bucket have hierarchical names. This means that, given a GCS URL of the form `gs://<bucket_name>/<series_uuid>/`, the user can take advantage of 'wildcarding' capabilities of various utiliaties to access the instances in the corresponding series. We will demonstrate.\n",
        "\n",
        "For this purpose, we will assume that the user has obtained such a GCS URL, specifically this URL:  \n",
        "`gs://crdcobj/f5d6b517-2c02-4035-9444-0f15be7180ff/`\n",
        "\n",
        "We expect that users will usually obtain such a GCS URL from the manifest of some IDC cohort or from some BQ query against a IDC BQ table such as dicom_all.\n",
        "\n",
        "Another way in which the user could have obtained such URL is by:\n",
        "1. resolving the series's DRS URI, which would in this case be `drs://dg.4DFC/f5d6b517-2c02-4035-9444-0f15be7180ff` (perhaps obtained from the manifest of an IDC cohort or from some BQ query against IDC BQ tables like dicom_all) to obtain a series compound object above\n",
        "2. Resolving the DRS URI in the 'folder_object' method (`drs://dg.4DFC/some_TBD_uuid' in the above compound object...we've not yet generated these UUIDs.)\n",
        "3. Resolving the DRS URI will return a DrsObject. An `AccessURL` in the DrsObject will have a `url` whose value is `gs://crdcobj/f5d6b517-2c02-4035-9444-0f15be7180ff/`\n",
        "\n",
        "As noted previously, see the CRDC compound object notebook for more details."
      ],
      "metadata": {
        "id": "eFFRPHdAz7FW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing the name hierarchy with gsutil\n",
        "Regardless of how the GCS URL of a series folder object was obtained, it can now be used with gsutil to access the instances in the series:"
      ],
      "metadata": {
        "id": "pRDxQiBgc0tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://crdcobj/f5d6b517-2c02-4035-9444-0f15be7180ff"
      ],
      "metadata": {
        "id": "jzMDTMy2xh_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a directory and copy all the instances in the series to the directory using gsutil."
      ],
      "metadata": {
        "id": "K7qZZq8u1YZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!mkdir -vp /tmp/dicom_data\n",
        "!gsutil -m cp gs://crdcobj/f5d6b517-2c02-4035-9444-0f15be7180ff/*.dcm /tmp/dicom_data\n",
        "# Demonstrated that we've copied the instances\n",
        "!ls -l /tmp/dicom_data\n",
        "# No longer needed. Delete them\n",
        "!rm -vr /tmp/dicom_data"
      ],
      "metadata": {
        "id": "W9fMwIJN4uK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing the name hierarchy using gcsfuse\n",
        "\n",
        "[gcsfuse](https://github.com/GoogleCloudPlatform/gcsfuse) is a [FUSE](https://en.wikipedia.org/wiki/Filesystem_in_Userspace) files system that allows you to mount a GCS bucket as a file system. Using gcsfuse we can access a series 'folder' in GCS as a directory.\n",
        "\n",
        "We first need to install gcsfuse:"
      ],
      "metadata": {
        "id": "siDD0Ihy6T47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "metadata": {
        "id": "MQnRwv9M6s4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a directory on which to mount the bucket, and perform the mount. "
      ],
      "metadata": {
        "id": "Sv-TQeSE64wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /tmp/gcsfuse_mnt\n",
        "!gcsfuse $demo_bucket /tmp/gcsfuse_mnt"
      ],
      "metadata": {
        "id": "Q9AvJLnJ6vG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can list the contents of that same series using the <series_uuid>:\n"
      ],
      "metadata": {
        "id": "1PswJZv99Qvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /tmp/gcsfuse_mnt/f5d6b517-2c02-4035-9444-0f15be7180ff/"
      ],
      "metadata": {
        "id": "AV13SWrJ8GyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with gsutil, we can copy all the instances in a series:"
      ],
      "metadata": {
        "id": "f62HfG6u-YcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!mkdir -vp /tmp/dicom_data\n",
        "!cp -v /tmp/gcsfuse_mnt/f5d6b517-2c02-4035-9444-0f15be7180ff/*.dcm /tmp/dicom_data\n",
        "!ls -l /tmp/dicom_data\n",
        "!rm -vr /tmp/dicom_data"
      ],
      "metadata": {
        "id": "Kbcvis5N-iM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing the name hierarchy using s5cmd\n",
        "The [s5cmd repo](https://github.com/peak/s5cmd) describes s5cmd as \"a very fast S3 and local filesystem execution tool\". But s5cmd can also be used against GCS. \n",
        "\n",
        "To use s5cmd, you must first create an HMAC. See [this](https://github.com/peak/s5cmd#google-cloud-storage-support) segment of the s5cmd documentation, which links you to [these](https://cloud.google.com/storage/docs/authentication/managing-hmackeys#create) instructions."
      ],
      "metadata": {
        "id": "DO6cls5syq8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keeping your AWS credentials in Google drive allows for convenient access. The following assumes AWS credentials are in /aws/credentials on the user's Google Drive"
      ],
      "metadata": {
        "id": "pqMlfMbedmVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!mkdir -p ~/.aws\n",
        "!cp /content/gdrive/MyDrive/aws/credentials ~/.aws"
      ],
      "metadata": {
        "id": "wf9I9vtly9ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "s5cmd can take a manifest of files to be transferred. We'll use gcsfuse to enumerate the instances in a series and then sed to the format required by s5cmd."
      ],
      "metadata": {
        "id": "pzFSToFJN5rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /tmp/gcsfuse_mnt/f5d6b517-2c02-4035-9444-0f15be7180ff/*.dcm | sed  \"s|/tmp/gcsfuse_mnt/|cp s3://$demo_bucket/|\" | sed \"s|\\(.*\\)|\\1 /tmp/dicom_data/.|\" > s5cmd_manifest.txt\n",
        "# !gsutil ls gs://crdcobj/f5d6b517-2c02-4035-9444-0f15be7180ff/*.dcm | sed  \"s|gs://|cp s3://|\" | sed \"s|\\(.*\\)|\\1 /tmp/dicom_data/.|\" > s5cmd_manifest.txt\n",
        "!cat s5cmd_manifest.txt"
      ],
      "metadata": {
        "id": "xowg4Vz1YRph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can pass the to s5cmd for processing:"
      ],
      "metadata": {
        "id": "Sx5rcrq3WlSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!mkdir -vp /tmp/dicom_data\n",
        "!./s5cmd --endpoint-url https://storage.googleapis.com run s5cmd_manifest.txt\n",
        "!ls -l /tmp/dicom_data\n",
        "!rm -vr /tmp/dicom_data"
      ],
      "metadata": {
        "id": "Xa0T3NNjWtqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the wall time, s5cmd is very fast."
      ],
      "metadata": {
        "id": "bdeSYPk2ZFYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing the name hierarchy using the Python GCS API\n",
        "\n",
        "The Google GCS APIs understand the implicit hierarchy in blob names. We can thus use it to access all the instance blobs of a series.\n",
        "\n",
        "First we define a function that we will use to \"walk\" the hierarchy.  \n",
        "The list_blobs() function is documented [here](https://googleapis.dev/python/storage/latest/storage/client.html?highlight=prefixes). Its *prefix* and *delimiter* parameters are used to emulate hierarchical naming. Basically it returns partial blob names for all blob names that begin with the specified *prefix* and up to the specified delimiter."
      ],
      "metadata": {
        "id": "0FRuDZVtD4Gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_blobs_with_prefix(prefix, delimiter=None):\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(demo_bucket)\n",
        "    blobs =  bucket.list_blobs(prefix=prefix, delimiter=delimiter)\n",
        "    names = [blob.name for blob in blobs]\n",
        "    ids = list(blobs.prefixes)\n",
        "    ids.sort()\n",
        "    return ids\n",
        "\n"
      ],
      "metadata": {
        "id": "3DTHFygvD2v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the <series_uuid> as the prefix to find all .dcm (instance) blobs:"
      ],
      "metadata": {
        "id": "Rn7V-_66eNY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instances= list_blobs_with_prefix(prefix='f5d6b517-2c02-4035-9444-0f15be7180ff', delimiter='.dcm')\n",
        "print(\"instance uuids:\")\n",
        "for instance in instances:\n",
        "  print(instances.index(instance), instance)\n"
      ],
      "metadata": {
        "id": "7merHd4tF-wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that have the `instances` list of instance blob names we can use other GCS functions to copy the contents of those blobs to a local directory:"
      ],
      "metadata": {
        "id": "RXbqTq578qHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from google.cloud.storage import Blob\n",
        "\n",
        "# Create a directory for the instance data:\n",
        "if not os.path.exists('/tmp/dicom_data'):\n",
        "    os.makedirs('/tmp/dicom_data')\n",
        "\n",
        "for instance in instances:\n",
        "  src_blob = bucket.blob(instance)\n",
        "  instance_blob_id = instance.split('/')[-1]\n",
        "  with open(f\"/tmp/dicom_data/{instance.split('/')[-1]}\", \"wb\") as file_obj:\n",
        "      src_blob.download_to_file(file_obj)\n",
        "\n",
        "# Demonstrate that we've downloaded the blobs.      \n",
        "!ls -l /tmp/dicom_data\n",
        "# Clean up\n",
        "shutil.rmtree('/tmp/dicom_data')"
      ],
      "metadata": {
        "id": "1ptQoh_cAesE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing instance data using the Python Requests library or curl\n",
        "Unlike the previous interfaces, requests, curl and wget do not support wildcarding. They each require a complete https URL in order to access an instance blob. That is, you can't use these APIs directly to get the instances in a series given just a bucket name and series_uuid. \n",
        "\n",
        "However, if an application really needs to use the Python Requests library, it can use the Google Storage library, as demonstrated above to get the full name of each instance in a series, and then use Requests to access the instance blob.\n",
        "\n",
        "We'll define a function for this purpose:\n",
        "\n"
      ],
      "metadata": {
        "id": "XdUFbkm_WyR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "access_url_prefix = f'https://storage.googleapis.com/{demo_bucket}'\n",
        "access_token = !gcloud auth print-access-token\n",
        "def get_blob_contents(instance):\n",
        "  url = f'{access_url_prefix}/{instance}'\n",
        "  headers = dict(Authorization=f'Bearer {access_token[0]}')\n",
        "  result = requests.get(url, headers=headers)\n",
        "  result.raise_for_status()\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "LKgZAa1NmwzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use requests.get to copy each instances in the previously created `instances` list to a local directory.\n",
        "\n",
        "Warning: This operation is very slow:"
      ],
      "metadata": {
        "id": "AWu6UGdJDAss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from google.cloud.storage import Blob\n",
        "\n",
        "# Create a directory for the instance data:\n",
        "if not os.path.exists('/tmp/dicom_data'):\n",
        "    os.makedirs('/tmp/dicom_data')\n",
        "\n",
        "bucket = client.get_bucket(demo_bucket)\n",
        "for instance in instances:\n",
        "  instance_blob_id = instance.split('/')[-1]\n",
        "  with open(f\"/tmp/dicom_data/{instance_blob_id}\", \"wb\") as file_obj:\n",
        "      result = get_blob_contents(instance)\n",
        "      file_obj.write(result.content)\n",
        "      print(f'Copied {instance}')\n",
        "# Demonstrate that we've downloaded the blobs.      \n",
        "!ls -l /tmp/dicom_data\n",
        "# Clean up\n",
        "shutil.rmtree('/tmp/dicom_data')"
      ],
      "metadata": {
        "id": "gsfMm8MwnEZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, a shell application that will use curl to copy blobs can use gsutil to get the names of instance blobs in a series.\n",
        "\n",
        "Warning: This operation is very slow:"
      ],
      "metadata": {
        "id": "b49BOF1cwsTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "%%bash -s \"$demo_bucket\" \"f5d6b517-2c02-4035-9444-0f15be7180ff\"\n",
        "mkdir -vp /tmp/dicom_data\n",
        "for instance in $(gsutil ls gs://$1/$2/*.dcm); \\\n",
        "do \\\n",
        "  IFS='/'; read -r -a parts <<< \"$instance\"; unset IFS; \\\n",
        "  bckt=\"${parts[2]}\"; series_uuid=\"${parts[3]}\"; instance_uuid=\"${parts[4]}\"; \\\n",
        "  URL=\"https://storage.googleapis.com/$bckt/$series_uuid/$instance_uuid?alt=media\"; \\\n",
        "  curl -s -X GET \\\n",
        "    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "    -o /tmp/dicom_data/$instance_uuid  \\\n",
        "    $URL; \\\n",
        "  echo Copied $instance; \\\n",
        "done\n",
        "# Demonstrate that we've downloaded the blobs.      \n",
        "ls -l /tmp/dicom_data\n",
        "# Clean up\n",
        "rm -vr /tmp/dicom_data"
      ],
      "metadata": {
        "id": "NoQOBlf_FU0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}